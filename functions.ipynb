{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGuM2CFg+JOINHhk5LPtwI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vl-Leschinskii/jokes_topics/blob/main/functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Функции использованные в статье, которые написаны самостоятельно, но в основном это готовые решения с просторов..."
      ],
      "metadata": {
        "id": "TkhLv1rhIvfu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Получение эмбеддингов из предобученной модели "
      ],
      "metadata": {
        "id": "4E2Trt_RM90_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_bert_cls(text, model, tokenizer):\n",
        "    t = tokenizer(text, padding=True, truncation=False, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        model_output = model(**{k: v.to(model.device) for k, v in t.items()})\n",
        "    embeddings = model_output.last_hidden_state[:, 0, :]\n",
        "    embeddings = torch.nn.functional.normalize(embeddings)\n",
        "    return embeddings[0].cpu().numpy()"
      ],
      "metadata": {
        "id": "diRMmd4tH0x5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Функция косинусной близости"
      ],
      "metadata": {
        "id": "4E0OMOUnNMtX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def similarity(v1, v2):\n",
        "    n1 = np.linalg.norm(v1)\n",
        "    n2 = np.linalg.norm(v2)\n",
        "\n",
        "    if n1 < 1e-6 or n2 < 1e-6:\n",
        "        return 0.0\n",
        "    else:\n",
        "        return np.dot(v1, v2) / n1 / n2\n",
        "\n",
        "def word_similarity(model, w1, w2):\n",
        "    return similarity(model.get_word_vector(w1), model.get_word_vector(w2))\n",
        "\n",
        "def sentence_similarity(model, t1, t2):\n",
        "    return similarity(model.get_sentence_vector(t1), model.get_sentence_vector(t2))"
      ],
      "metadata": {
        "id": "u-JNUVL4H0Gp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Функции визуализации топиков"
      ],
      "metadata": {
        "id": "twzBakiCNXP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_topics_gensim(model, feature_names, no_top_words):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "      print(\"Topic %d:\" % (topic_idx))\n",
        "      print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))"
      ],
      "metadata": {
        "id": "In2hko1EubfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_topics_sklearn(model, feature_names, no_top_words):\n",
        "    topic_list=[]\n",
        "    no_top_words=1000  \n",
        "    for topic_idx, topics in enumerate(model.components_):\n",
        "      sort_topix=topics.argsort()[:-no_top_words - 1:-1]\n",
        "      to_idx=[topic_idx]    \n",
        "      top_list=[]\n",
        "      print(\"Topic %d:\" % (topic_idx))\n",
        "      line=[]       \n",
        "      for i in sort_topix:\n",
        "        top_list=[]       \n",
        "        top_list.append(topic_idx)\n",
        "        top_list.append(feature_names[i])\n",
        "        top_list.append(topics[i])\n",
        "        topic_list.append(top_list)\n",
        "        line.append(feature_names[i])        \n",
        "      print(line[:10])           \n",
        "    return pd.DataFrame(topic_list,columns=['topic','topic_word','topic_val'])#,topic_list_2"
      ],
      "metadata": {
        "id": "dbAK1MWpOBf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Строит визуализацию облака слов топиков для модели **Gensim**."
      ],
      "metadata": {
        "id": "KdouWE8oIkCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plotWordCloud_Gensim(topic_number,topics,head):\n",
        "    text = dict(topics[topic_number][1])\n",
        "    wordcloud = WordCloud(background_color=\"white\", max_words=100, width=900, height=900, collocations=False)\n",
        "    wordcloud = wordcloud.generate_from_frequencies(text)\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    plt.title((head+\" номер {}\").format(topic_number))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\");"
      ],
      "metadata": {
        "id": "QaQEa2tMcpVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Строит визуализацию облака слов топиков для модели **Sklearn**."
      ],
      "metadata": {
        "id": "G4jcggMBJK-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# облако слов топиков для модели sklearn\n",
        "def plotWordCloud_Sklearn(topic_number,topics):\n",
        "    # получаем частоты и слова топика\n",
        "    topic=topic_number\n",
        "    words=topics[topics['topic']==topic]['topic_word'].values.astype('U')\n",
        "    long_string =','.join(words)\n",
        "    # строим облако слов\n",
        "    wordcloud = WordCloud(background_color=\"white\", max_words=100, width=900, height=900, collocations=False)\n",
        "    wordcloud.generate(long_string)\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    plt.title(\"Топик номер {}\".format(topic_number))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\");"
      ],
      "metadata": {
        "id": "kUE7EKvwpqp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Natural Language Toolkit"
      ],
      "metadata": {
        "id": "AS7TByo4N_oX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk_download(\"punkt\")\n",
        "!wget https://raw.githubusercontent.com/dhhse/dh2020/master/data/stop_ru.txt\n",
        "with open (\"stop_ru.txt\", \"r\") as stop_ru:\n",
        "    rus_stops = [word.strip() for word in stop_ru.readlines()]\n",
        "punctuation = '!\\\"#$%&\\'()*+,-./:;<=>?@[\\]^_`{|}~—»«...–'    \n",
        "filter_token = rus_stops + list (punctuation)\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "parser = MorphAnalyzer()"
      ],
      "metadata": {
        "id": "Dnf-q1RqW4lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Функции для предобработки текста. \n",
        "\n",
        "Слова приводятся к нижнему регистру,\n",
        "стоп-слова удаляются, далее слова лемматизируются\n",
        "\n",
        "\n",
        "input_text - Входной текст для очистки и лемматизации\n",
        "\n",
        "return\n",
        "  out_text - Очищенный и лемматизированный текст, как последовательность разделенных пробелами токенов\n",
        "  lemmatized_text - Очищенный и лемматизированный текст, как список токенов"
      ],
      "metadata": {
        "id": "lWr-DCXXKI6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sklearn(input_text):\n",
        "    text = input_text.lower()\n",
        "    tokenized_text = word_tokenize(text)\n",
        "    clean_text = [word for word in tokenized_text if word not in filter_token]\n",
        "    lemmatized_text = [parser.parse(word)[0].normal_form for word in clean_text]\n",
        "    out_text = \" \".join(lemmatized_text)    \n",
        "    return out_text\n",
        "\n",
        "def preprocess_gemsim(input_text):\n",
        "    text = input_text.lower()\n",
        "    tokenized_text = word_tokenize(text)\n",
        "    clean_text = [word for word in tokenized_text if word not in filter_token]\n",
        "    lemmatized_text = [parser.parse(word)[0].normal_form for word in clean_text]    \n",
        "    return lemmatized_text"
      ],
      "metadata": {
        "id": "aXkNaCEeLkxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Функция назначает документу наиболее вероятный топик\n",
        "\n",
        "*words* -  лемматизированный текст документа\n",
        "\n",
        "*lda* -  тематическая модель\n",
        "\n",
        "*return* -  список из наиболее вероятного топика и его вероятности"
      ],
      "metadata": {
        "id": "wwl3Udn6LwTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_topic(words, lda):\n",
        "    bag = lda.id2word.doc2bow(words)\n",
        "    topics = lda.get_document_topics(bag)\n",
        "    topic_dictionary = {}\n",
        "    for topic in topics:\n",
        "        topic_dictionary[topic[1]] = str((topic[0])) \n",
        "    main_probability = max(topic_dictionary)\n",
        "    main_topic = topic_dictionary[main_probability]\n",
        "    return [main_topic, main_probability]"
      ],
      "metadata": {
        "id": "rNl_eIuNmTLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Получение lda модели в библиотеке **Gensim**"
      ],
      "metadata": {
        "id": "lCeg6dghMRBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lda_model_gensim(corpus,dictionary,num_topics):\n",
        "  ldamodel = models.ldamodel.LdaModel(\n",
        "    corpus, \n",
        "    id2word=dictionary, \n",
        "    eval_every=20, \n",
        "    num_topics=num_topics, \n",
        "    passes=5\n",
        "    )\n",
        "  return ldamodel"
      ],
      "metadata": {
        "id": "cO80-MELwn1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Функции для сохранения топиков в файл"
      ],
      "metadata": {
        "id": "FLhKlyHdMl5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def topic_file(topics_gensim_np,file_name):\n",
        "  list_topic=[]\n",
        "  for ind_topiс,topiс in enumerate(topics_gensim_np):\n",
        "    list_word=topiс[1]\n",
        "    for words in list_word:\n",
        "      topic_word=[]\n",
        "      topic_word.append(ind_topiс)\n",
        "      topic_word.append(words[0])\n",
        "      topic_word.append(words[1])\n",
        "      list_topic.append(topic_word)\n",
        "  topic_gensim=pd.DataFrame(list_topic,columns=['topic','word_in topic','weght_topic'])\n",
        "  topic_gensim.to_csv(file_name+'.csv',index=False)\n",
        "  return topic_gensim"
      ],
      "metadata": {
        "id": "ueajAE7PdY5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def topics_max(mod_data,model):\n",
        "  top_n=[]\n",
        "  for i in range(mod_data.shape[0]):\n",
        "#for i in range(100):\n",
        "    top_max=[]\n",
        "    topic_list=model.transform(tfidf[i,:])\n",
        "    top_max.append(np.max(topic_list))\n",
        "    top_max.append(np.argmax(topic_list))  \n",
        "    top_n.append(top_max)\n",
        "  return np.array(top_n)"
      ],
      "metadata": {
        "id": "sRNG33HYdJa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Генератор анекдотов"
      ],
      "metadata": {
        "id": "tTL4EifKF0D1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def joke_generator(data,topic_id,num,topic_model='topic_lda',):\n",
        "  if topic_model=='topic_base': \n",
        "    data_topic=topic_base\n",
        "  if topic_model=='topic_bi': \n",
        "    data_topic=topic_bi\n",
        "  if topic_model=='topic_nmf': \n",
        "    data_topic=topic_nmf\n",
        "  if topic_model=='topic_lda': \n",
        "    data_topic=topic_lda\n",
        "  word_in_topic=data_topic[data_topic['topic']==topic_id]['topic_word'].values[:10]\n",
        "  print('токены топика:',\", \".join(word_in_topic) )\n",
        "  print()\n",
        "\n",
        "#  plotWordCloud_2(topic_number=topic_id, topics=data_topic)\n",
        "  jokes=data[data[topic_model]==topic_id]['text_base'].values  \n",
        "  for i, joke in enumerate(jokes[:num]):\n",
        "    print('анекдот',i,joke)"
      ],
      "metadata": {
        "id": "ElAixEMnF22x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Построение биграмм"
      ],
      "metadata": {
        "id": "rlcmCybfYeo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_bigrams(texts):\n",
        "    return [bigram_mod[doc] for doc in texts]"
      ],
      "metadata": {
        "id": "1BEPJEbnSdKr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}